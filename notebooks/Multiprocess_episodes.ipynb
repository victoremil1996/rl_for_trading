{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762da113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Magic commands\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "Python standard packages\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import statsmodels.api as sm\n",
    "\"\"\"\n",
    "Own packages\n",
    "\"\"\"\n",
    "sys.path.insert(0, \"C:\\\\code\\\\speciale\\\\\") # vesl path\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\lucas\\\\OneDrive\\\\Skrivebord\\\\repo\\\\speciale\\\\\") # ljb path\n",
    "\n",
    "from market_simulation_study.agent import RandomAgent, InvestorAgent, TrendAgent, RLAgent, MarketMakerAgent\n",
    "from market_simulation_study.agent import Memory, MuPolicyNetwork, ActionValueNetwork, GaussianPolicyNetwork, GaussianBinomialPolicyNetwork, ActorCriticAgent\n",
    "from market_simulation_study.environment import MarketEnvironment\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from codelib.plotting import dist_vs_normal_plot, volume_contribution_plot, DefaultStyle, default_colors\n",
    "sns.set_theme(\"paper\",\"whitegrid\")\n",
    "DefaultStyle()\n",
    "\"\"\"\n",
    "Initializations\n",
    "\"\"\"\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7dc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare parameters\n",
    "n_random_agents = 4\n",
    "n_investor_agents = 2\n",
    "n_trend_agents = 4\n",
    "n_mm_agents = 15\n",
    "n_rl_agents = 1\n",
    "\n",
    "n_agents = n_random_agents + n_investor_agents + n_trend_agents + n_rl_agents + n_mm_agents\n",
    "\n",
    "time_periods = 1000\n",
    "\n",
    "price_list = [100 + np.random.normal(scale = 0.01) for j in range(100)]\n",
    "ex_list = np.array([[0]*n_agents, [0]*n_agents])\n",
    "fee = 0\n",
    "slippage = 0\n",
    "mean_buy_price = 99.5\n",
    "mean_sell_price = 100.5\n",
    "delta = 1\n",
    "atp = 0\n",
    "total_buy_volume = 0\n",
    "total_sell_volume = 0\n",
    "\n",
    "state0 = {\"market_prices\": price_list, \"volume\": ex_list, 'fee': fee, \"mean_buy_price\": mean_buy_price,\n",
    "          \"mean_sell_price\": mean_sell_price, 'slippage': slippage, \n",
    "          'total_buy_volume': total_buy_volume, 'total_sell_volume': total_sell_volume}#, 'all_traded_prices': atp}\n",
    "\n",
    "def reset1():\n",
    "    agents = []\n",
    "\n",
    "    investor_agents = []\n",
    "\n",
    "    inv_intensity = 0.02\n",
    "\n",
    "    if n_investor_agents >= 1:\n",
    "        investor_agents.append(InvestorAgent(agent_id = 0,\n",
    "                                         delta = delta,\n",
    "                                         intensity = inv_intensity / 4,\n",
    "                                         buy_price_margin = 0.0025,\n",
    "                                         sell_price_margin = 0.010,\n",
    "                                         buy_volume = 15,\n",
    "                                         sell_volume = 30, \n",
    "                                         n_orders = 10))\n",
    "    if n_investor_agents == 2:  \n",
    "        investor_agents.append(InvestorAgent(agent_id = 1,\n",
    "                                             delta = delta,\n",
    "                                             intensity = inv_intensity / 2,\n",
    "                                             n_orders = 6,\n",
    "                                             buy_price_margin  = 0.005,\n",
    "                                             sell_price_margin = 0.020,\n",
    "                                             buy_volume = 25,\n",
    "                                             sell_volume = 50,\n",
    "                                             can_short = True))\n",
    "\n",
    "    random_agents = [RandomAgent(agent_id = j + n_investor_agents,\n",
    "                                 delta = delta,\n",
    "                                 noise_range = (0.0001, 0.0003),\n",
    "                                 mid_price_noise = 0.0025,\n",
    "                                 n_coin_flips = 3, \n",
    "                                 coin_bias_sell = 0.5,\n",
    "                                 coin_bias_buy = 0.5) for j in range(n_random_agents)]\n",
    "\n",
    "    trend_agents = [TrendAgent(agent_id = j + n_random_agents + n_investor_agents,\n",
    "                                     delta = delta,\n",
    "                                      moving_average_one = np.random.randint(10, 15),\n",
    "                                      moving_average_two = np.random.randint(20, 30),\n",
    "                                      price_margin = 0.005) for j in range(n_trend_agents)]\n",
    "\n",
    "    mm_agents = [MarketMakerAgent(agent_id = j + n_random_agents + n_investor_agents + n_trend_agents,\n",
    "                                  delta = delta,\n",
    "                                  gamma = 0.00005,\n",
    "                                  gamma2 = np.random.uniform(0.5, 0.75), #np.random.randint(1, 1),\n",
    "                                  spread_zero = np.random.uniform(0.1, 0.1), \n",
    "                                  n_volume = 3) for j in range(n_mm_agents)]\n",
    "\n",
    "\n",
    "    agents.append(investor_agents)\n",
    "    agents.append(random_agents)\n",
    "    agents.append(trend_agents)\n",
    "    agents.append(mm_agents)\n",
    "\n",
    "    return agents\n",
    "\n",
    "agents = reset1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e663f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# PARAMETERS\n",
    "#####################\n",
    "\"\"\"Neural Networks\"\"\"\n",
    "max_action_value = 0.035  # Prices\n",
    "min_action_value = -0.035  # Prices\n",
    "max_action_value_two = 5  # Volumes\n",
    "min_action_value_two = 0  # Volumes\n",
    "\n",
    "action_dims = 2\n",
    "state_dims = 12\n",
    "fc1_dims = 256\n",
    "fc2_dims = 256\n",
    "learning_rate = 3e-7\n",
    "norm_sigma = 0.005\n",
    "n_gradient_steps_per_update = 1\n",
    "update_every = 10\n",
    "batch_size = 8\n",
    "\n",
    "\"\"\"RL Agent\"\"\"\n",
    "discount_factor = 0.95\n",
    "agent_id = n_random_agents + n_investor_agents + n_trend_agents + n_mm_agents\n",
    "def reset2(agents):\n",
    "    ######################\n",
    "    # AGENT INITIALIZATION\n",
    "    ######################\n",
    "    # policy = GaussianPolicyNetwork(max_action_value     = max_action_value,\n",
    "    #                                min_action_value     = min_action_value,\n",
    "    #                                max_action_value_two = max_action_value_two,\n",
    "    #                                min_action_value_two = min_action_value_two,\n",
    "    #                                input_dims           = state_dims,\n",
    "    #                                action_dims          = action_dims,\n",
    "    #                                fc1_dims             = fc1_dims,\n",
    "    #                                fc2_dims             = fc2_dims)\n",
    "\n",
    "    policy = MuPolicyNetwork(max_action_value     = max_action_value,\n",
    "                                   min_action_value     = min_action_value,\n",
    "                                   max_action_value_two = max_action_value_two,\n",
    "                                   min_action_value_two = min_action_value_two,\n",
    "                                   input_dims           = state_dims,\n",
    "                                   action_dims          = action_dims,\n",
    "                                   fc1_dims             = fc1_dims,\n",
    "                                   fc2_dims             = fc2_dims,\n",
    "                                   sigma                = norm_sigma,\n",
    "                                   name                 = 'policy_last_dropout_new_loss_1205NOSAVE',\n",
    "                                   n_volume = 3,\n",
    "                                   dropout = 0.2\n",
    "                                   )\n",
    "\n",
    "    policy_optimiser = Adam(policy.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    qf = ActionValueNetwork(input_dims = state_dims + action_dims,\n",
    "                            fc1_dims = fc1_dims,\n",
    "                           fc2_dims = fc2_dims,\n",
    "                           name = 'qf_last_dropout_new_loss_1205NOSAVE')\n",
    "    qf_optimiser = Adam(qf.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    vf = ActionValueNetwork(input_dims = state_dims,\n",
    "                            name = 'vf_last_dropout_new_loss_1205NOSAVE')\n",
    "    vf_optimiser = Adam(vf.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    rl_agents = [ActorCriticAgent(policy           = policy, \n",
    "                                  policy_optimiser = policy_optimiser,\n",
    "                                  qf               = qf,\n",
    "                                  qf_optimiser     = qf_optimiser,\n",
    "                                  vf               = vf,\n",
    "                                  vf_optimiser     = vf_optimiser, \n",
    "                                  discount_factor  = discount_factor, \n",
    "                                  agent_id         = agent_id,\n",
    "                                  init_state       = state0,\n",
    "                                  batch_size       = batch_size,\n",
    "                                 position_penalty  = 0.1)]\n",
    "\n",
    "    agents.append(rl_agents)\n",
    "    \n",
    "    agents = [item for sublist in agents for item in sublist]\n",
    "    \n",
    "    return agents\n",
    "\n",
    "agents = reset2(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = 500\n",
    "n_episodes = 1500\n",
    "rl_profit = []\n",
    "market_prices = []\n",
    "inv1_pos = []\n",
    "inv2_pos = []\n",
    "market_volume = []\n",
    "atps = np.array([np.nan])\n",
    "stack_autocorrels = False\n",
    "\n",
    "save_count = 10\n",
    "mm_volumes = np.zeros(int(time_periods / save_count))\n",
    "random_volumes = np.zeros(int(time_periods / save_count))\n",
    "investor_volumes = np.zeros(int(time_periods / save_count))\n",
    "trend_volumes = np.zeros(int(time_periods / save_count))\n",
    "rl_volumes = np.zeros(int(time_periods / save_count))\n",
    "\n",
    "mm_pnls = np.zeros(int(time_periods / save_count))\n",
    "random_pnls = np.zeros(int(time_periods / save_count))\n",
    "investor_pnls = np.zeros(int(time_periods / save_count))\n",
    "trend_pnls = np.zeros(int(time_periods / save_count))\n",
    "rl_pnls = np.zeros(int(time_periods / save_count))\n",
    "\n",
    "bps = []\n",
    "sps = []\n",
    "\n",
    "#pd.DataFrame([stacked_volumes, stacked_volumes])\n",
    "all_traded_volumes = np.zeros((n_episodes, 5, int(time_periods / save_count)))\n",
    "all_total_volumes = np.zeros((n_episodes, int(time_periods / save_count)))\n",
    "all_pnls_volumes = np.zeros((n_episodes, 5, int(time_periods / save_count)))\n",
    "all_policy_loss = []\n",
    "all_qf_loss = []\n",
    "all_vf_loss = []\n",
    "all_rl_end_rewards = np.zeros(n_episodes)\n",
    "all_mus_1 = []\n",
    "all_mus_2 = []\n",
    "all_rl_positions = np.zeros((n_episodes, time_periods))\n",
    "old_data = np.load('rl_market_experiment_data.npy',allow_pickle='TRUE').item()\n",
    "old_rf_data = old_data.copy()\n",
    "\n",
    "overwrite_old = False\n",
    "\n",
    "def run_episode(episode):\n",
    "    agents = reset1()\n",
    "    agents = reset2(agents)\n",
    "    agents[-1].load_models(file_qf = \"nn_models/qf_last_dropout_new_loss\",\n",
    "                          file_vf = \"nn_models/vf_last_dropout_new_loss\",\n",
    "                          file_policy = \"nn_models/policy_last_dropout_new_loss\")\n",
    "#    for episode in range(n_episodes):\n",
    "    mm_volumes = np.zeros(int(time_periods / save_count))\n",
    "    random_volumes = np.zeros(int(time_periods / save_count))\n",
    "    investor_volumes = np.zeros(int(time_periods / save_count))\n",
    "    trend_volumes = np.zeros(int(time_periods / save_count))\n",
    "    rl_volumes = np.zeros(int(time_periods / save_count))\n",
    "\n",
    "    mm_pnls = np.zeros(int(time_periods / save_count))\n",
    "    random_pnls = np.zeros(int(time_periods / save_count))\n",
    "    investor_pnls = np.zeros(int(time_periods / save_count))\n",
    "    trend_pnls = np.zeros(int(time_periods / save_count))\n",
    "    rl_pnls = np.zeros(int(time_periods / save_count))\n",
    "    # TO REDUCE RUNTIME RESET AGENTS AND LOAD MODELS\n",
    "#     if episode % 2 == 0 and episode != 0:\n",
    "#         all_vf_loss.append(agents[-1].vf_loss_mem)\n",
    "#         all_qf_loss.append(agents[-1].qf_loss_mem)\n",
    "#         all_policy_loss.append(agents[-1].policy_loss_mem)\n",
    "#         agents = reset1()\n",
    "#         agents = reset2(agents)\n",
    "#         #agents[-1].load_models(\"nn_models/policy_nn_nn\", \"nn_models/qf_nn_nn\", \"nn_models/vf_nn_nn\")\n",
    "#         agents[-1].load_models()\n",
    "\n",
    "    agents[-1].reset()\n",
    "    env = MarketEnvironment(state0, use_last_traded_price = True)\n",
    "    # Initialize agents\n",
    "    for j in range(n_agents):\n",
    "            agents[j].reset()\n",
    "            agents[j].update(state0)\n",
    "\n",
    "    # START EPISODE\n",
    "    rl_profit = []\n",
    "    rl_reward = []\n",
    "    rl_positions = []\n",
    "    agent_class = []\n",
    "    rl_buy_prices = []\n",
    "    rl_sell_prices = []\n",
    "    rl_buy_volume = []\n",
    "    rl_sell_volume = []\n",
    "    market_prices = []\n",
    "    mus = np.array([[0, 0, 0, 0]])\n",
    "    for time in range(time_periods):\n",
    "\n",
    "        agents, state = env.step(agents)\n",
    "\n",
    "        for j in range(n_agents):\n",
    "            agents[j].update(state)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # RL AGENT UPDATE\n",
    "        #################################\n",
    "        if time % 10 == 0:# or episode < 10:\n",
    "            agents[-1].update(state, exploration_mode = True)\n",
    "        else:\n",
    "            agents[-1].update(state, exploration_mode = False)\n",
    "\n",
    "        # Store RL DATA\n",
    "        agent_class.append(agents[-1].agent_class)\n",
    "        rl_profit.append(agents[-1].pnl)\n",
    "        rl_reward.append(agents[-1].memory.rewards[-1].detach().numpy())\n",
    "        rl_positions.append(agents[-1].position)\n",
    "        rl_sell_prices.append(agents[-1].sell_price)\n",
    "        rl_buy_prices.append(agents[-1].buy_price)\n",
    "        rl_sell_volume.append(agents[-1].sell_volume)\n",
    "        rl_buy_volume.append(agents[-1].buy_volume)\n",
    "\n",
    "        # mus\n",
    "        #mus.append([agents[-1].mu1, agents[-1].mu2, agents[-1].mu3, agents[-1].mu4])\n",
    "        mus = np.vstack((mus, [agents[-1].mu1.detach().numpy(), agents[-1].mu2.detach().numpy(), \n",
    "                               3, 3]))\n",
    "\n",
    "        #########################################\n",
    "        # SAVE VOLUMES\n",
    "        #########################################\n",
    "\n",
    "        if save_count:\n",
    "\n",
    "            if time % save_count == 0:\n",
    "                for agent in agents:\n",
    "                    agent.calculate_profit_and_loss(state)\n",
    "                    if agent.agent_class == \"MM\":\n",
    "                        mm_volumes[int(time / save_count)] += np.abs(agent.all_trades[:, 1]).sum()\n",
    "                        mm_pnls[int(time / save_count)] += agent.pnl\n",
    "                    elif agent.agent_class == \"Random\":\n",
    "                        random_volumes[int(time / save_count)] += np.abs(agent.all_trades[:, 1]).sum()\n",
    "                        random_pnls[int(time / save_count)] += agent.pnl\n",
    "                    elif agent.agent_class == \"Investor\":\n",
    "                        investor_volumes[int(time / save_count)] += np.abs(agent.all_trades[:, 1]).sum()\n",
    "                        investor_pnls[int(time / save_count)] += agent.pnl\n",
    "                    elif agent.agent_class == \"Trend\":\n",
    "                        trend_volumes[int(time / save_count)] += np.abs(agent.all_trades[:, 1]).sum()\n",
    "                        trend_pnls[int(time / save_count)] += agent.pnl\n",
    "                    elif agent.agent_class == \"ActorCritic\":\n",
    "                        rl_volumes[int(time / save_count)] += np.abs(agent.all_trades[:, 1]).sum()\n",
    "                        rl_pnls[int(time / save_count)] += agent.pnl\n",
    "\n",
    "        #########################################\n",
    "        # UPDATE PLOTS\n",
    "        #########################################\n",
    "\n",
    "        inv1_pos.append(agents[0].position)\n",
    "        inv2_pos.append(agents[1].position)\n",
    "        market_volume.append(state[\"volume\"])\n",
    "        market_prices.append(state[\"market_prices\"][-1])\n",
    "\n",
    "    #########################################\n",
    "    # END OF EPISODE\n",
    "    #########################################\n",
    "    if stack_autocorrels:\n",
    "        mp = pd.DataFrame(atps)\n",
    "        returns = (mp / mp.shift(1)  - 1).dropna()\n",
    "        if episode == 0:\n",
    "            all_stacked_returns = returns.values.flatten()\n",
    "        else:\n",
    "            all_stacked_returns = np.hstack((all_stacked_returns, returns.values.flatten()))\n",
    "\n",
    "\n",
    "\n",
    "    mm_volumes[1:] = mm_volumes[1:] - mm_volumes[:-1]\n",
    "    random_volumes[1:] = random_volumes[1:] - random_volumes[:-1]\n",
    "    investor_volumes[1:] = investor_volumes[1:] - investor_volumes[:-1]\n",
    "    trend_volumes[1:] = trend_volumes[1:] - trend_volumes[:-1]\n",
    "    rl_volumes[1:] = rl_volumes[1:] - rl_volumes[:-1]\n",
    "    total_volumes = mm_volumes + investor_volumes + trend_volumes + random_volumes + rl_volumes\n",
    "\n",
    "    mm_volumes = mm_volumes / total_volumes\n",
    "    investor_volumes = investor_volumes / total_volumes\n",
    "    trend_volumes = trend_volumes / total_volumes\n",
    "    random_volumes = random_volumes / total_volumes\n",
    "    rl_volumes = rl_volumes / total_volumes\n",
    "\n",
    "    stacked_volumes = np.vstack((investor_volumes, trend_volumes, random_volumes, mm_volumes, rl_volumes))\n",
    "    stacked_pnls = np.vstack((investor_pnls / n_investor_agents, trend_pnls / n_trend_agents, \n",
    "                              random_pnls / n_random_agents, mm_pnls / n_mm_agents,\n",
    "                             rl_pnls))\n",
    "\n",
    "    all_traded_volumes[episode,:,:] = stacked_volumes\n",
    "    all_pnls_volumes[episode,:,:] = stacked_pnls\n",
    "    all_total_volumes[episode,:] = total_volumes\n",
    "    rl_end_rewards = np.sum(agents[-1].memory.rewards[:]).detach().numpy()[0]\n",
    "    all_rl_end_rewards[episode] = np.sum(agents[-1].memory.rewards[:]).detach().numpy()[0]\n",
    "    all_rl_positions[episode,:] = rl_positions\n",
    "\n",
    "    out = {'volumes': all_traded_volumes, 'pnls': all_pnls_volumes,\n",
    "          'end_rewards': all_rl_end_rewards, 'vf_loss': all_vf_loss,\n",
    "          'qf_loss': all_qf_loss, 'policy_loss': all_policy_loss,\n",
    "          'positions': all_rl_positions, 'total_volumes': all_total_volumes}\n",
    "\n",
    "    #################################\n",
    "    # VISUALISATION\n",
    "    #################################\n",
    "\n",
    "    pol_loss = all_policy_loss\n",
    "    pol_loss = [item for sublist in pol_loss for item in sublist]\n",
    "\n",
    "    qf_loss = all_qf_loss\n",
    "    qf_loss = [item for sublist in qf_loss for item in sublist]\n",
    "\n",
    "    vf_loss = all_vf_loss\n",
    "    vf_loss = [item for sublist in vf_loss for item in sublist]\n",
    "\n",
    "    return stacked_volumes, stacked_pnls, total_volumes, rl_end_rewards, rl_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755125d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "res = run_episode(1)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f6edb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_map\n\u001b[0;32m      2\u001b[0m no_episodes_to_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      3\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "no_episodes_to_run = 5\n",
    "t0 = time.time()\n",
    "results = process_map(run_episode, range(no_episodes_to_run = 2), max_workers = 5)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(\"time: \", total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
